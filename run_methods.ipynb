{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee3019b9-8f3f-46c5-b80e-ca7cac139f06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T19:54:47.247216Z",
     "iopub.status.busy": "2024-12-20T19:54:47.246118Z",
     "iopub.status.idle": "2024-12-20T19:54:56.532628Z",
     "shell.execute_reply": "2024-12-20T19:54:56.531759Z",
     "shell.execute_reply.started": "2024-12-20T19:54:47.247169Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataset import SourceImgDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# cat\n",
    "dataset = SourceImgDataset('/home/jupyter/datasphere/project/edm/datasets/afhqv2-64x64', lbl_val=0)\n",
    "train_loader = DataLoader(dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# wild\n",
    "tgt_dataset = SourceImgDataset('/home/jupyter/datasphere/project/edm/datasets/afhqv2-64x64', lbl_val=1)\n",
    "tgt_loader = DataLoader(tgt_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84c65c9b-f310-42cd-8152-b83fa3303814",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T19:54:56.535496Z",
     "iopub.status.busy": "2024-12-20T19:54:56.534139Z",
     "iopub.status.idle": "2024-12-20T19:55:10.327137Z",
     "shell.execute_reply": "2024-12-20T19:55:10.326332Z",
     "shell.execute_reply.started": "2024-12-20T19:54:56.535458Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/resources/edm\n",
      "/home/jupyter/work/resources\n"
     ]
    }
   ],
   "source": [
    "%cd edm\n",
    "from dnnlib import util\n",
    "import torch_utils\n",
    "%cd ..\n",
    "import pickle\n",
    "\n",
    "# load pretrained diffusion\n",
    "device = 'cuda:0'\n",
    "with util.open_url('wild64.pkl') as f:\n",
    "    net = pickle.load(f)['ema'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b8b3f0-4686-48e4-8123-71f7f2dca1cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T01:23:49.914697Z",
     "iopub.status.busy": "2024-12-20T01:23:49.913435Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/resources/edm\n",
      "/home/jupyter/work/resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1024 images saved: 100%|██████████| 1024/1024 [00:03<00:00, 277.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Inception-v3 model...\n",
      "Loading images from \"orig_imgs\"...\n",
      "Calculating statistics for 1024 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.47s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /tmp/xdg_cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:09<00:00, 60.1MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/jupyter/.local/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 3/22 [09:37<1:00:59, 192.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Inception-v3 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from \"gen_imgs\"...\n",
      "Calculating statistics for 1024 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.28s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/jupyter/.local/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 3/22 [17:07<1:48:26, 342.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Inception-v3 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from \"gen_imgs\"...\n",
      "Calculating statistics for 1024 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.26s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/jupyter/.local/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# SDEDIT\n",
    "\n",
    "%cd edm\n",
    "from fid import calculate_inception_stats, calculate_fid_from_inception_stats\n",
    "from dnnlib.util import open_url\n",
    "%cd ..\n",
    "\n",
    "from metrics import compute_metrics_and_save_imgs, save_model_samples\n",
    "import json\n",
    "\n",
    "sampling_params = {\n",
    "    'device': 'cuda',\n",
    "    'sigma_min': 0.02,\n",
    "    'sigma_max': 10.0,\n",
    "    'num_steps': 10,\n",
    "    'rho': 7.0,\n",
    "    'vis_steps': 1,\n",
    "    'stochastic': False,\n",
    "    'cfg': 0\n",
    "}\n",
    "\n",
    "exp_results = {}\n",
    "\n",
    "batch = 512\n",
    "num_samples = 1024\n",
    "orig_path = 'orig_imgs'\n",
    "gen_path = 'gen_imgs'\n",
    "save_model_samples(orig_path, tgt_loader, num_samples)\n",
    "mu_real, sigma_real = calculate_inception_stats(image_path=orig_path, num_expected=num_samples, max_batch_size=batch)\n",
    "\n",
    "sigmas = [5, 10, 25, 40]\n",
    "steps = [18, 32, 50]\n",
    "\n",
    "for sigma in sigmas:\n",
    "    for step in steps:\n",
    "        exp_name = 'sigma={:.1f};n_steps={}'.format(sigma, step)\n",
    "\n",
    "        sampling_params['sigma_max'] = sigma\n",
    "        sampling_params['num_steps'] = step\n",
    "        res_json = compute_metrics_and_save_imgs(gen_path, train_loader, 'sdedit', net, sampling_params, to_see=num_samples)\n",
    "        \n",
    "        mu_gen, sigma_gen = calculate_inception_stats(image_path=gen_path, num_expected=num_samples, max_batch_size=batch)\n",
    "        fid = calculate_fid_from_inception_stats(mu_gen, sigma_gen, mu_real, sigma_real)\n",
    "        \n",
    "        res_json['FID'] = fid\n",
    "        exp_results[exp_name] = res_json\n",
    "        \n",
    "        with open('sdedit_results.json', 'w') as f:\n",
    "            json.dump(exp_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15709e9d-d9e0-489f-bf36-95c5991510c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f5fd7-61c2-43f8-a4f5-8fce2ddf56e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e72b07-eafc-419a-aa8e-b3940eac4fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b57a4-7586-4015-8174-8972e1bc7db5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ILVR\n",
    "\n",
    "%cd edm\n",
    "from fid import calculate_inception_stats, calculate_fid_from_inception_stats\n",
    "from dnnlib.util import open_url\n",
    "%cd ..\n",
    "\n",
    "from metrics import compute_metrics_and_save_imgs, save_model_samples\n",
    "import json\n",
    "\n",
    "sampling_params = {\n",
    "    'device': 'cuda',\n",
    "    'sigma_min': 0.02,\n",
    "    'sigma_max': 80.0,\n",
    "    'num_steps': 10,\n",
    "    'rho': 7.0,\n",
    "    'vis_steps': 1,\n",
    "    'stochastic': False,\n",
    "    'cfg': 0,\n",
    "    'scale_factor': 2\n",
    "}\n",
    "\n",
    "exp_results = {}\n",
    "\n",
    "batch = 512\n",
    "num_samples = 1024\n",
    "#orig_path = 'orig_imgs'\n",
    "gen_path = 'gen_imgs'\n",
    "#save_model_samples(orig_path, tgt_loader, num_samples)\n",
    "#mu_real, sigma_real = calculate_inception_stats(image_path=orig_path, num_expected=num_samples, max_batch_size=batch)\n",
    "\n",
    "Ns = [4, 8, 16, 32]\n",
    "steps = [18, 32, 50]\n",
    "\n",
    "for N in Ns:\n",
    "    for step in steps:\n",
    "        exp_name = 'N={};n_steps={}'.format(N, step)\n",
    "\n",
    "        sampling_params['scale_factor'] = N\n",
    "        sampling_params['num_steps'] = step\n",
    "        res_json = compute_metrics_and_save_imgs(gen_path, train_loader, 'ilvr', net, sampling_params, to_see=num_samples)\n",
    "        \n",
    "        mu_gen, sigma_gen = calculate_inception_stats(image_path=gen_path, num_expected=num_samples, max_batch_size=batch)\n",
    "        fid = calculate_fid_from_inception_stats(mu_gen, sigma_gen, mu_real, sigma_real)\n",
    "        \n",
    "        res_json['FID'] = fid\n",
    "        exp_results[exp_name] = res_json\n",
    "        \n",
    "        with open('ilvr_results.json', 'w') as f:\n",
    "            json.dump(exp_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d482169d-9132-470e-aa53-316e6df3678c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c40190c-183e-4629-82c1-0cc65723cd59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eef08b-257f-402f-beca-d91e1f639b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60644253-0888-4783-8862-f37ee4f566c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EGSDE\n",
    "\n",
    "# Uncomment this to train classifier model\n",
    "\n",
    "# from train_classifier import train_loop\n",
    "# from dataset import CombinedImgDataset\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# dataset = CombinedImgDataset('/home/jupyter/datasphere/project/edm/datasets/afhqv2-64x64')\n",
    "\n",
    "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [9634, 1000])\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# class_model = class_model.to('cuda')\n",
    "# classifier_model = train_loop(train_loader, val_loader, class_model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe669971-012f-47a4-8f3d-9f2093e09da5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T19:55:10.328545Z",
     "iopub.status.busy": "2024-12-20T19:55:10.328072Z",
     "iopub.status.idle": "2024-12-20T19:55:10.367235Z",
     "shell.execute_reply": "2024-12-20T19:55:10.366466Z",
     "shell.execute_reply.started": "2024-12-20T19:55:10.328512Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/resources/guided-diffusion\n",
      "/home/jupyter/work/resources\n"
     ]
    }
   ],
   "source": [
    "%cd guided-diffusion\n",
    "from guided_diffusion.unet import EncoderUNetModel\n",
    "from guided_diffusion.nn import timestep_embedding\n",
    "%cd ..\n",
    "\n",
    "class EGClassifier(EncoderUNetModel):\n",
    "    def forward(self, x, timesteps):\n",
    "        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n",
    "\n",
    "        h = x.type(self.dtype)\n",
    "        for module in self.input_blocks:\n",
    "            h = module(h, emb)\n",
    "        res = self.middle_block(h, emb)\n",
    "        h_res = res.type(x.dtype)\n",
    "        return res, self.out(h_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebc04a8a-b67a-425c-840b-f9ec8004c7f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T19:55:10.369439Z",
     "iopub.status.busy": "2024-12-20T19:55:10.368879Z",
     "iopub.status.idle": "2024-12-20T19:55:10.942683Z",
     "shell.execute_reply": "2024-12-20T19:55:10.941923Z",
     "shell.execute_reply.started": "2024-12-20T19:55:10.369408Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_model = EGClassifier(image_size=64,\n",
    "                           in_channels=3,\n",
    "                           out_channels=1000,\n",
    "                           model_channels=128,\n",
    "                           channel_mult=(1,2,3,4),\n",
    "                           attention_resolutions=[32, 16, 8],\n",
    "                           use_scale_shift_norm=True,\n",
    "                           resblock_updown=True,\n",
    "                           num_res_blocks=4,\n",
    "                           num_head_channels=64,\n",
    "                           pool='attention'\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf7aa51d-b5a9-4fbe-beb0-ad4ce91dc35b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T19:55:10.944135Z",
     "iopub.status.busy": "2024-12-20T19:55:10.943633Z",
     "iopub.status.idle": "2024-12-20T19:55:26.957428Z",
     "shell.execute_reply": "2024-12-20T19:55:26.956682Z",
     "shell.execute_reply.started": "2024-12-20T19:55:10.944102Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "pretrained_dict = torch.load('64x64_classifier.pt', map_location=\"cuda\")\n",
    "model_dict = class_model.state_dict()\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "model_dict.update(pretrained_dict)\n",
    "class_model.load_state_dict(model_dict, strict=False)\n",
    "\n",
    "class_model.out[2].c_proj = nn.Conv1d(512, 2,  kernel_size=(1,), stride=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "261929e9-b374-4b27-a508-750058d142de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T19:55:26.958885Z",
     "iopub.status.busy": "2024-12-20T19:55:26.958420Z",
     "iopub.status.idle": "2024-12-20T19:55:42.849269Z",
     "shell.execute_reply": "2024-12-20T19:55:42.848545Z",
     "shell.execute_reply.started": "2024-12-20T19:55:26.958853Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrained_class = torch.load('checkpoints/checkpoint_epoch_9', map_location=\"cuda\")\n",
    "class_model.load_state_dict(pretrained_class)\n",
    "\n",
    "# need to lower batch size\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283c33ac-74eb-4173-8505-1c5fff4a120a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T19:55:42.850960Z",
     "iopub.status.busy": "2024-12-20T19:55:42.850282Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/resources/edm\n",
      "/home/jupyter/work/resources\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /tmp/xdg_cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:05<00:00, 107MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/jupyter/.local/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/174 [01:27<1:01:31, 21.71s/it]"
     ]
    }
   ],
   "source": [
    "%cd edm\n",
    "from fid import calculate_inception_stats, calculate_fid_from_inception_stats\n",
    "from dnnlib.util import open_url\n",
    "%cd ..\n",
    "\n",
    "from metrics import compute_metrics_and_save_imgs, save_model_samples\n",
    "import json\n",
    "\n",
    "sampling_params = {\n",
    "    'device': 'cuda',\n",
    "    'sigma_min': 0.02,\n",
    "    'sigma_max': 80.0,\n",
    "    'num_steps': 10,\n",
    "    'rho': 7.0,\n",
    "    'vis_steps': 1,\n",
    "    'stochastic': False,\n",
    "    'cfg': 0,\n",
    "    'scale_factor': 2,\n",
    "    'class_model': class_model,\n",
    "    'l_1': 2,\n",
    "    'l_2': 500\n",
    "}\n",
    "\n",
    "exp_results = {}\n",
    "\n",
    "batch = 512\n",
    "num_samples = 1024\n",
    "#orig_path = 'orig_imgs'\n",
    "gen_path = 'gen_imgs'\n",
    "#save_model_samples(orig_path, tgt_loader, num_samples)\n",
    "#mu_real, sigma_real = calculate_inception_stats(image_path=orig_path, num_expected=num_samples, max_batch_size=batch)\n",
    "\n",
    "sigmas = [10, 25]\n",
    "Ns = [8, 16, 32]\n",
    "steps = [18, 32]\n",
    "\n",
    "for sigma in sigmas:\n",
    "    for N in Ns:\n",
    "        for step in steps:\n",
    "            exp_name = 'sigma={};N={};n_steps={}'.format(sigma, N, step)\n",
    "\n",
    "            sampling_params['scale_factor'] = N\n",
    "            sampling_params['num_steps'] = step\n",
    "            sampling_params['sigma_max'] = sigma\n",
    "            res_json = compute_metrics_and_save_imgs(gen_path, train_loader, 'egsde', net, sampling_params, to_see=num_samples)\n",
    "\n",
    "            mu_gen, sigma_gen = calculate_inception_stats(image_path=gen_path, num_expected=num_samples, max_batch_size=batch)\n",
    "            fid = calculate_fid_from_inception_stats(mu_gen, sigma_gen, mu_real, sigma_real)\n",
    "\n",
    "            res_json['FID'] = fid\n",
    "            exp_results[exp_name] = res_json\n",
    "\n",
    "            with open('egsde_results.json', 'w') as f:\n",
    "                json.dump(exp_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
